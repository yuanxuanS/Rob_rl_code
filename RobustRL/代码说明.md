# 说明

## 修改1

在一个图上：play多个episode

- for episodes:
  - for steps:
    - 

​	



# class Environment:

```python
def __init__(self, T, sub_T, budget, graph, node_feat_dim):
```

<mark>self.G</mark> graph , Graph_IM instance

<mark>self.g</mark>: self.G.graph，nx生成的图，

run_cascade()

- 进行ICcascade模拟需要输入该图

<mark>self.N</mark>: self.G.node， int，节点个数

__init()

- 初始化传播概率矩阵，定义其行列数

init_state()

- 初始化二维状态，状态的列数为节点个数。

generate_node_feature():

- 产生节点特征矩阵，为特征矩阵行数。

<mark>self.propagate_p_matrix</mark> : numpy array, 二维，n*n，每条边的传播概率

reset()

- 用hyper_mode()重新初始化

hyper_model()

- 用计算出的所有边的传播概率，构造这个传播概率矩阵

run_cascade()

- 进行ICcascade模拟需要输入该传播矩阵

<mark>self.adj_matrix</mark>: nested list，二维n*n，self.G.adj_matrix，邻接矩阵

<mark>self.node_features</mark>: 初始化为None, 通过generate_node_feature()生成节点特征， numpy array， 二维 [<mark>self.N</mark>, <mark>self.node_feat_dimension</mark>]

reset()  

- 重新生成节点特征

generate_edge_features()   gen_edge_fea()

- 拼接两个节点特征=边特征时，索引对应的下标

<mark>self.z</mark>: 初始化为None, numpy array， 一维,  2*<mark>self.node_feat_dimension</mark>

reset()  

- 重新生成超参

get_z_state()

- 获取环境超参

step_hyper()

- 输入z增量，更新超参

init_hyper_z()

- 初始化超参z，numpy array， 一维,  2*<mark>self.node_feat_dimension</mark>

hyper_model()

- 和所有的边特征相乘，取均值得到每条边的传播系数

<mark>self.edge_features</mark>, nested list，每个边是节点特征拼起来

<mark>self.node_feat_dimension</mark>: int, 节点的特征维度，自定义。

generate_node_feature():

- 产生节点特征矩阵，为特征矩阵列数。

init_hyper_z()

- 产生超参z，维度为 2*self.node_feat_dimension, 因为要乘这么大的边特征

<mark>self.T</mark>: int, 一个nature agent 的episode的步数

<mark>self.sub_T</mark>: int，一个main agent的episode步数

<mark>self.budgt</mark>: int, 预算

<mark>self.state</mark>： 

init_state()

- 初始化， numpy array, 二维， 1*self.N

get_seed_state()

- 遍历，选出可选择的节点=0

transition()

- 把给定的action节点状态记录，变为1，即为加入种子集

step_seed()

- 从state判断，获取种子集

# class Graph_IM:

```python
def __init__(self, nodes, edges_p, seed=0)
```

### <mark>self.graph</mark>: 用nx库生成的图， 用到nodes和edges_p，seed

## node()-@property

返回int，用nx得到的图的节点数

## nodes()-@property

返回list，图上的节点的数字下标列表

## edges()-@property

返回list，tuples，（start node, end node）

## adj_matrix()- @property

返回nested list，二维邻接矩阵

# class DQAgent

- agent.py

```python
def __init__(self, graph, lr, node_features_dims, node_features, model_name, 			init_epsilon, train_batch, episode_T, update_target_steps):
```

#### **graph** :

object, 自定义的<mark>Graph_IM</mark> instance, 赋值给<mark>self.graph</mark>

act()

- policy_model() 求q_a, 需要输入self.graph.adj_matrix
- 通过self.graph.node 来得到infeasible_action

update()

- target_model()  求target，需要输入self.graph.adj_matrix
- policy_model() 求q_a, 需要输入self.graph.adj_matrix

#### <mark>lr</mark> : 

float, 优化器optimizer的学习率

#### **node_features_dims**: 

int, 节点特征的维度，赋值给<mark>self.node_features_dims</mark> 

__init():

定义model，赋值给feature_dim

- GAT() model作为policy model和target model的输入特征维度nfeat

#### **node_features**: 

numpy array, env.node_features，n*node_features_dims, n个节点的特征。赋值给<mark>self.node_features</mark>

act()

- 通过深拷贝，作为policy_model()的输入计算q_a。（因为在policy_model中会融合state如seed set特征改变输入，为了不改变原来的节点特征，这里必须用Deepcopy）

update()

- 用batch更新policy时，作为target_model输入，计算target  <mark>?</mark>deepcopy?
- 作为policy_model()的输入计算q_a  <mark>?</mark>也需要deepcopy?

#### model_name

string, model/algorithm 名称，用于选择模型算法

#### init_epsilon

float, 赋值给<mark>self.curr_epsilon</mark>

act()

- epsilon-greedy policy的参数

#### train_batch

int， 赋值给<mark>self.train_batch_size</mark>，每次训练网络的样本数（比如从buffer中采样的样本数量）

get_sample():

- 如果memory中样本数量足够，从memory中采样这么多的样本  <mark>？</mark>deepcopy

#### episode_T

int，赋值给<mark>self.episode_steps</mark>，一个episode迭代的step数量

update():

- DQN算法，用样本更新网络时，用于判断是否到达terminate，如果到达terminate，target=reward

#### update_target_steps

int，赋值给<mark>self.copy_model_steps</mark>，DQN算法中对target网络进行参数更新的step间隔

update():

- 如果现在iter_step是<mark>self.copy_model_steps</mark>整数倍，把target_model参数更新为policy_model的参数



# class PPOContinuousAgent

```python
def __init__(self, graph, lr, node_features_dims, node_features, model_name,
                 gamma, lmbda, eps, epochs, train_batch, episode_T, update_target_steps):
```

#### **self.graph** :

object, 自定义的<mark>Graph_IM</mark> instance

### self.actor_lr, self.critic_lr

float, 网络的学习率

#### self.node_features_dims

int, 节点特征维度

__init()

- 初始化中actor网络输入特征的维度，输出为二倍大小，和z大小一样
-  critic网络输入特征的维度

#### self.node_features

numpy array， 二维

act()

- 作为actor网络的输入

update()

- 作为actor， critic的输入，每次网络调用都要输入节点特则会那个

#### self.actor

GATPolicyNet object

__init()

- 初始化
- 定义好优化器，训练actor参数

actor()

- 输入参数z，得到action的分布，对应位置根据对应的mu, std采样得到每个位置上的action值，所以最后只需要

update()

- 输入states得到分布，采样得到old action
- on policy 迭代多次更新网络，
  - 输入states得到分布，采样得到new action
  - old & new action得到比率
  - 计算两个代理目标，从而得到actor loss

#### class critic

GATValueNet，object

__init()

- 初始化
- 定义好优化器，训练critic参数

update()

- 输入states和next_state计算得到td_delta
- on policy 迭代多次更新网络，
  - 计算critic loss = MSE(critic(states) - td_target)

#### self.memory

list

reset()

- 初始化为[]

remeber()

- 存一个样本

update()

- 获取其中的样本，用于训练

#### self.gamma

float, 计算td_target和Advantage的参数

#### self.lmbda

float, 计算Advantage的参数

#### self.eps

float, 剪切目标的参数

#### self.epochs

float, update（） 训练网络次数的参数

#### update()

- 先计算td_target，critic评价现在的states，得到差距。actor输入现在states下的动作分布
- 多次迭代actor critic网络
  - 新actor输入现在states下的动作分布，得到两个代理目标值，计算loss，更新
  - 新critic评价现在的states下的value，和td_target计算loss, 更新
  - **和DQN这样的离散有限动作空间不同，通过代理目标来评价好坏**。



## class GAT

- models.py

```python
def __init__(self, nfeat, nhid, nout, dropout, alpha, nheads):
    # 多个attention层叠在一起的GAT网络，对特征向量进行更新
```

#### <mark>nfeat</mark>： 

int，每个attention层的输入特征矩阵的大小

__init()：

- 用于定义GraphAttentionLayers的参数
- 用于定义融合seed set的参数<mark>self.theta</mark>大小，<mark>self.theta</mark>是一个一维向量，大小为<mark>nfeat</mark>，如果属于种子集则该节点的输入特征加上该参数。

#### <mark>nhid</mark>:

int，每个attention层的输出特征矩阵的大小

__init()：

- 用于定义GraphAttentionLayers的参数
- GAT拼接多头Layer的结果为 n\*(nhead\*nhid)，最后为了降维多头attention结果，用<mark>out_att</mark>，该网络层的输入大小，nhid*nheads

#### <mark>nout</mark>

int，最后特征矩阵的维度/列数

__init()：

- 用于定义<mark>out_att</mark>的参数

#### <mark>dropout</mark>

float，dropout参数

__init()：

- 所有attention层的dropout参数

forward():

- 输入特征矩阵，先经过一个dropout，参数
- 多头attention拼接后，再经过一次dropout

#### alpha

float，非线性激活函数Leakrelu的负数部分的斜率参数

__init()

- 所有attention层的非线性激活参数

#### <mark>nheads</mark>

int，多头个数

#### forward(x, adj, observation, add_seed_set)

- 输入特征矩阵x，[n, <mark>nfeat</mark>]，图的邻接矩阵adj [n,n], 观察到的状态observation这里是环境返回的state，add_seed_set bool是否融合observation信息
- 更新的特征矩阵h_prime [n, <mark>nout</mark>]

## class GraphAttentionLayer

- layers.py

```python
def __init__(self, in_features, out_features, dropout, alpha, concat=True):
    # 对特征向量的提取了图结构特征的一次更新，
```

#### in_features 

int, 赋值给 <mark>self.in_features</mark>, 输入特征量x的维度大小/列数

__init():

- 定义参数矩阵W的行数

#### out_features

int，赋值给<mark>self.out_features</mark>，该Layer的输出量，或者说输出特征的维度大小/列数

__init():

- 定义参数矩阵W的列数

- 定义function a也就是矩阵a的行数。求两个节点之间的注意力系数，把两个节点的特征经过W映射（<mark>self.out_features</mark>长）拼接起来，用a映射成一个实数。所以a的行数为 2*<mark>self.out_features</mark>

_prepare_attentional_mechanism_input（）

- 求两个节点之间的注意力系数，把两个节点的特征经过W映射（<mark>self.out_features</mark>长）--Wh，拼接起来，用a映射成一个实数。这时先把a的前半段/后半段和Wh相乘然后广播达到目的，这里是一个trick

#### dropout

float，赋值给<mark>self.dropout</mark>,dropout的概率值

forward()

- 前向传播时，在计算完所有节点的attention系数后，对attention矩阵进行一个dropout。

#### alpha

float，赋值给<mark>self.alpha</mark>，非线性激活函数Leakrelu的负数部分的斜率参数

__init()

- 定义leakyrelu时的参数，计算attention系数后进行一下非线性激活

#### concat

bool，赋值给<mark>self.concat</mark>，多头拼接时=True

forward():

- 如果=True，多头拼接，要通过一个elu非线性激活  ？？？

#### forward(h, adj):

- 输入特征矩阵h，[n, <mark>self.in_features</mark>]，图的邻接矩阵adj [n,n]
- 输出经过注意力提取后更新的特征矩阵h_prime [n, <mark>self.out_features</mark>]

